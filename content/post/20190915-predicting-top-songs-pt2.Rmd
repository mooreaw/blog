---
title: "predicting my yearly top songs without listening/usage data (part 2)"
author: Andrew
date: '2019-09-23'
slug: "top-songs-over-time-spotifyr-2"
tags: ["spotifyr", "R", "tidymodels", "supervised learning"]
draft: true
---

```{r opts, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, echo = FALSE
)
```

```{r setup and import, echo = TRUE}
set.seed(20190914)
options(yardstick.event_first = FALSE)


library(tidyverse)
library(tidymodels)
library(zeallot)
library(scales)

theme_set(
  theme_minimal(base_size = 18) +
    theme(panel.grid.minor = element_blank())
)

tracks <- read_csv("../../static/data/predicting-top-songs/20190915-ts-tracks-train-test.csv")
```

```{r}
tracks <- tracks %>%
  mutate(
    is_target     = factor(is_target),
    playlist_year = factor(playlist_year),
    
    keygroup = case_when(
      key_mode %in% c(
        "D# minor", "B major", "G minor", "C minor", "A# major"
      ) ~ "great",
      key_mode %in% c(
        "C# minor", "F# major", "A major", "D minor", "G major", "A minor",
        "C major"
      ) ~ "good",
      key_mode %in% c(
        "E minor", "A# minor"
      ) ~ "not good",
      TRUE ~ "fine"
    )
  )

tracks <- tracks %>%
  split(tracks$dataset) %>%
  map(~select(., -dataset, -time_signature, -playlist_name, -playlist_img)) 

c(test, train) %<-% tracks

ts_cvdat <- vfold_cv(train)
```

```{r}
ts_recipe <- function(dataset, skip_toggle = TRUE, r = 1) {
  # vector of each of the numeric variables
  numerics <- c(
    "danceability", "energy", "loudness", "speechiness", "liveness",
    "acousticness", "instrumentalness",
    "valence", "tempo",  "duration_ms"
  )
  
  # the full formula of variables to be included for modeling
  varspec <- is_target ~
    danceability       +
    energy             +
    loudness           +
    speechiness        +
    liveness           +
    acousticness       +
    instrumentalness   +
    valence            +
    tempo              +
    duration_ms        +
    keygroup           +
    playlist_year      +
    playlist_mon
  
  # with our specified variables, create dummies for the year/mon and keygroup
  # then perform a PCA on the numeric columns (after centering/scaling)
  recipe(varspec, data = dataset) %>%
    step_upsample(is_target, ratio = r, skip = skip_toggle) %>%
    step_dummy(keygroup, playlist_year, playlist_mon) %>%
    step_center(one_of(numerics)) %>%
    step_scale(one_of(numerics)) %>%
    step_pca(one_of(numerics), num_comp = 3) %>%
    step_interact(terms = ~PC1:PC2) %>%
    step_interact(terms = ~PC1:PC3) %>%
    step_interact(terms = ~PC2:PC3)
}

ts_logit <- function(split, id, r = .7) {
  tr <- analysis(split)
  ts <- assessment(split)
  
  tr_prep <- prep(ts_recipe(tr, FALSE, r = r), training = tr)
  tr_proc <- bake(tr_prep, new_data = tr)
  
  ts_prep <- prep(ts_recipe(ts), testing = ts)
  ts_proc <- bake(ts_prep, new_data = ts)
  
  model <- logistic_reg("classification") %>%
    set_engine("glm") %>%
    fit(is_target ~ ., data = tr_proc)
  
  tibble(
    `id`  = id,
    truth = ts_proc$is_target,
    pred  = unlist(predict(model, ts_proc)) 
  )
}

ts_rf <- function(split, id, ntree = 1000, r = .7) {
  tr <- analysis(split)
  ts <- assessment(split)
  
  tr_prep <- prep(ts_recipe(tr, FALSE, r = r), training = tr)
  tr_proc <- bake(tr_prep, new_data = tr)
  
  ts_prep <- prep(ts_recipe(ts), testing = ts)
  ts_proc <- bake(ts_prep, new_data = ts)
  
  model <- rand_forest("classification", trees = ntree) %>%
    set_engine("ranger") %>%
    fit(is_target ~ ., data = tr_proc)
  
  tibble(
    `id`  = id,
    truth = ts_proc$is_target,
    pred  = unlist(predict(model, ts_proc)) 
  )
}

ts_knn <- function(split, id, nn = 4, r = .7) {
  tr <- analysis(split)
  ts <- assessment(split)
  
  tr_prep <- prep(ts_recipe(tr, FALSE, r = r), training = tr)
  tr_proc <- bake(tr_prep, new_data = tr)
  
  ts_prep <- prep(ts_recipe(ts), testing = ts)
  ts_proc <- bake(ts_prep, new_data = ts)
  
  model <- nearest_neighbor("classification", nn) %>%
    set_engine("kknn") %>%
    fit(is_target ~ ., data = tr_proc)
  
  tibble(
    `id`  = id,
    truth = ts_proc$is_target,
    pred  = unlist(predict(model, ts_proc)) 
  )
}

log <- map2_df(.x = ts_cvdat$splits, .y = ts_cvdat$id, ~ts_logit(.x, .y))
rf  <- map2_df(.x = ts_cvdat$splits, .y = ts_cvdat$id, ~ts_rf(.x, .y))
knn <- map2_df(.x = ts_cvdat$splits, .y = ts_cvdat$id, ~ts_knn(.x, .y))

class_metrics <- metric_set(ppv, npv, sens, spec, accuracy)

lst(log, rf, knn) %>%
  bind_rows(.id = "model") %>%
  group_by(model, id) %>%
  class_metrics(truth = truth, estimate = pred) %>%
  group_by(model, .metric) %>%
  summarise_at(vars(.estimate), list(median, sd)) %>%
  gather(desc, val, fn1, fn2) %>%
  ggplot(aes(x = .metric, y = val, fill = model)) +
  geom_col(position = "dodge") +
  facet_wrap(~desc)
```
