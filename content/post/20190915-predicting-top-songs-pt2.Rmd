---
title: "predicting my yearly top songs without listening/usage data (part 2)"
author: Andrew
date: '2019-09-23'
slug: "top-songs-over-time-spotifyr-2"
tags: ["spotifyr", "R", "tidymodels", "supervised learning"]
draft: true
---

```{r opts, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, echo = FALSE
)
```

```{r setup and import, echo = TRUE}
set.seed(20190914)
options(yardstick.event_first = FALSE)


library(tidyverse)
library(tidymodels)
library(zeallot)
library(scales)

theme_set(
  theme_minimal(base_size = 18) +
    theme(panel.grid.minor = element_blank())
)

tracks <- read_csv("../../static/data/predicting-top-songs/20190915-ts-tracks-train-test.csv")
```

```{r}
tracks <- tracks %>%
  mutate(
    is_target     = factor(is_target),
    playlist_year = factor(playlist_year),
    
    keygroup = case_when(
      key_mode %in% c(
        "D# minor", "B major", "G minor", "C minor", "A# major"
      ) ~ "great",
      key_mode %in% c(
        "C# minor", "F# major", "A major", "D minor", "G major", "A minor",
        "C major"
      ) ~ "good",
      key_mode %in% c(
        "E minor", "A# minor"
      ) ~ "not good",
      TRUE ~ "fine"
    )
  )

tracks <- tracks %>%
  split(tracks$dataset) %>%
  map(~select(., -dataset, -time_signature, -playlist_name, -playlist_img)) 

c(test, train) %<-% tracks

ts_cvdat <- vfold_cv(train)
```

```{r}
ts_recipe <- function(dataset, skip_toggle = TRUE, r = .7) {
  # the full formula of variables to be included for modeling
  varspec <- is_target ~ keygroup + playlist_year + playlist_mon
  
  # up-sample our target class in order to even out the class imbalance
  # with our specified variables, create dummies for the year/mon and keygroup
  recipe(varspec, data = dataset) %>%
    step_upsample(is_target, ratio = r, skip = skip_toggle) %>%
    step_dummy(keygroup, playlist_year, playlist_mon)
}

ts_logit <- function(split, id, r = .7) {
  tr <- analysis(split)
  ts <- assessment(split)
  
  tr_prep <- prep(ts_recipe(tr, FALSE, r = r), training = tr)
  tr_proc <- bake(tr_prep, new_data = tr)
  
  ts_prep <- prep(ts_recipe(ts), testing = ts)
  ts_proc <- bake(ts_prep, new_data = ts)
  
  model <- logistic_reg("classification") %>%
    set_engine("glm") %>%
    fit(is_target ~ ., data = tr_proc)
  
  tibble(
    `id`  = id,
    truth = ts_proc$is_target,
    pred  = unlist(predict(model, ts_proc)) 
  )
}

ts_rf <- function(split, id, ntree = 1000, r = .7) {
  tr <- analysis(split)
  ts <- assessment(split)
  
  tr_prep <- prep(ts_recipe(tr, FALSE, r = r), training = tr)
  tr_proc <- bake(tr_prep, new_data = tr)
  
  ts_prep <- prep(ts_recipe(ts), testing = ts)
  ts_proc <- bake(ts_prep, new_data = ts)
  
  model <- rand_forest("classification", trees = ntree) %>%
    set_engine("ranger") %>%
    fit(is_target ~ ., data = tr_proc)
  
  tibble(
    `id`  = id,
    truth = ts_proc$is_target,
    pred  = unlist(predict(model, ts_proc)) 
  )
}

ts_knn <- function(split, id, nn = 4, r = .7) {
  tr <- analysis(split)
  ts <- assessment(split)
  
  tr_prep <- prep(ts_recipe(tr, FALSE, r = r), training = tr)
  tr_proc <- bake(tr_prep, new_data = tr)
  
  ts_prep <- prep(ts_recipe(ts), testing = ts)
  ts_proc <- bake(ts_prep, new_data = ts)
  
  model <- nearest_neighbor("classification", nn) %>%
    set_engine("kknn") %>%
    fit(is_target ~ ., data = tr_proc)
  
  tibble(
    `id`  = id,
    truth = ts_proc$is_target,
    pred  = unlist(predict(model, ts_proc)) 
  )
}

log <- map2_df(.x = ts_cvdat$splits, .y = ts_cvdat$id, ~ts_logit(.x, .y))
rf  <- map2_df(.x = ts_cvdat$splits, .y = ts_cvdat$id, ~ts_rf(.x, .y))
knn <- map2_df(.x = ts_cvdat$splits, .y = ts_cvdat$id, ~ts_knn(.x, .y))

class_metrics <- metric_set(ppv, npv, sens, spec, accuracy)

lst(log, rf, knn) %>%
  bind_rows(.id = "model") %>%
  group_by(model, id) %>%
  class_metrics(truth = truth, estimate = pred) %>%
  group_by(model, .metric) %>%
  summarise_at(vars(.estimate), list(median, sd)) %>%
  gather(desc, val, fn1, fn2) %>%
  ggplot(aes(x = .metric, y = val, fill = model)) +
  geom_col(position = "dodge") +
  facet_wrap(~desc)
```

```{r stacked models}
ts_stacked <- function(split, id) {
  tr <- analysis(split)
  ts <- assessment(split)
  
  tr_prep <- prep(ts_recipe(tr, FALSE, r = .7), training = tr)
  tr_proc <- bake(tr_prep, new_data = train)
  
  ts_prep <- prep(ts_recipe(ts), testing = ts)
  ts_proc <- bake(ts_prep, new_data = ts)
  
  rf <- rand_forest("classification", trees = 1000) %>%
    set_engine("ranger") %>%
    fit(is_target ~ ., data = tr_proc)
  
  lg <- logistic_reg("classification") %>%
    set_engine("glm") %>%
    fit(is_target ~ ., data = tr_proc)
  
  kn <- nearest_neighbor("classification", 4) %>%
    set_engine("kknn") %>%
    fit(is_target ~ ., data = tr_proc)
  
  tr_proc <- mutate(
    tr_proc,
    rf    = unlist(predict(rf, tr_proc)),
    lg    = unlist(predict(lg, tr_proc)),
    kn    = unlist(predict(kn, tr_proc))
  )
  
  ts_proc <- mutate(
    ts_proc,
    rf    = unlist(predict(rf, ts_proc)),
    lg    = unlist(predict(lg, ts_proc)),
    kn    = unlist(predict(kn, ts_proc))
  )
  
  model <- rand_forest("classification", trees = 5000) %>%
    set_engine("ranger") %>%
    fit(is_target ~ ., data = tr_proc)
  
  tibble(
    `id`  = id,
    truth = ts_proc$is_target,
    pred  = unlist(predict(model, ts_proc)) 
  )
}

ts_stacked %>%
  group_by(id) %>%
  class_metrics(truth, estimate = pred) %>%
  group_by(.metric) %>%
  summarise_at(vars(.estimate), list(mean, median, sd))
```
