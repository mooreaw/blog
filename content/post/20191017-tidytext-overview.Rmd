---
title: "analyzing the october primary debate, using tidytext"
author: Andrew
date: '2019-11-25'
slug: "tidytext-overview"
tags: ["tidytext", "R"]
# output:
#   blogdown::html_page:
#     highlight: pygments
---

```{r opts, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, echo = TRUE
)
```

```{r pull-dem-debate-transcript, include = FALSE, eval = FALSE}
library(tidyverse)
library(rvest)

# link to WaPo's transcript
wp <- read_html("https://www.washingtonpost.com/politics/2019/10/15/october-democratic-debate-transcript/")

# save the raw html
write_rds(wp, "static/post/20191017-tidytext-overview/data/20191110-wp-html.rds")

wp_txt <- tibble(
  # parse the html, dropping instances where the audience is making noise
  txt = wp %>%
    html_nodes(".undefined") %>%
    html_text() %>%
    str_trim() %>%
    discard(~ .x %in% c("(APPLAUSE)", "(CROSSTALK)", "(LAUGHTER)", "(COMMERCIAL BREAK)", "END", "")),
  
  # speakers are identified by all-caps names at the the start of the line
  speaker = txt %>%
    str_replace_all("O'ROURKE", "OROURKE") %>%
    str_extract("^([A-Z]+?):|^\\(UNKNOWN\\):") %>%
    str_to_title() %>%
    str_remove(":") %>%
    str_replace_all("Orourke", "O'Rourke"),
  
  # tag where we are in the debate
  index = cumsum(!is.na(speaker))
)

# remove the speaker names from their text
names  <- "(BIDEN|BOOKER|BURNETT|BUTTIGIEG|CASTRO|COOPER|GABBARD|HARRIS|KLOBUCHAR|LACEY|O'ROURKE|SANDERS|STEYER|WARREN|YANG): "
wp_txt <- mutate(wp_txt, txt = str_remove(txt, names))

# if there's a gap between speakers, assume it's who spoke last
for (i in 4:nrow(wp_txt)) {
  if (is.na(wp_txt[[i, 2]])) wp_txt[[i, 2]] <- wp_txt[[i - 1, 2]]
}

# compress each comment into a single observation
wp_txt <- wp_txt %>%
  group_by(speaker, index) %>%
  summarise(txt = str_c(txt, collapse = " ")) %>%
  ungroup() %>%
  arrange(index)

write_rds(wp_txt, "static/post/20191017-tidytext-overview/data/20191110-oct-dem-debate-cleaned.rds")
```

(This is a write-up of a talk I gave to the Ann Arbor R User Group, earlier this month.)

It seems like the longer one works with data, the probability they are tasked to work with *unstructured text* approaches 1. No matter the setting --whether you're working with survey responses, administrative data, whatever-- one of the most common ways that humans record information is by writing things down. Something great about text-based data is that it's often plentiful, and might have the advantage of being really descriptive of something you're trying to study. However, the path to summarizing the information contained within this kind of data can feel daunting, especially if you're encountering it for the first time.

With this perspective in mind, I want to write down a few of the things I've learned about the excellent R package, [**_tidytext_**](https://github.com/juliasilge/tidytext){target="_blank"}, which follows principles of the **_tidyverse_** in encouraging the use of [*tidy*](https://en.wikipedia.org/wiki/Tidy_data){target="_blank"} data (c.f. [Wickham, 2014](https://www.jstatsoft.org/article/view/v059i10){target="_blank"}). Julia Silge and David Robinson (the authors of **_tidytext_**) have also [written a book](https://www.tidytextmining.com/){target="_blank"} on using the package in combination with other tools in the R language to analyze text. The full-text is free online, and you can purchase a paper copy through O'Reilly. If you find this post useful, I would recommend moving onto their book as a more thorough guide, with many useful examples.

## what we'll do in this post

I'm aiming for 3 things:

1. Basic vocabulary around text analysis, as related to **_tidytext_** functions
2. Demonstrate tokenization and pre-processing of text
3. Describe some textual data (details below)

For this exercise, I've pulled down the transcript of the October democratic party primary debate from 10/15/2019. I wanted to work with something fresh, and for this post we can imagine ourselves as a data journalist looking to describe patterns of speech from different candidates.

## fundamental units of text: tokens

Our first piece of vocabulary: "token". A *token* is a meaningful sequence of characters, such as a word, phrase, or sentence. One of the main tasks of mining text-data is converting strings of characters into the types of tokens needed for analysis; this process is called tokenization. As far as how this is accomplished with **_tidytext_**, the workflow is to create a table (data.frame/tibble) with a single token per row. We accomplish this step using the `unnest_tokens()` function.

```{r, echo = TRUE}
library(tidyverse)
library(tidytext)
library(scico)

# load the debate transcript data
wp <- read_rds("../../static/post/20191017-tidytext-overview/data/20191110-oct-dem-debate-cleaned.rds")

# Cooper's introduction to the debate
wp[[2, 3]] %>% str_sub(1, 128)

dd_uni <- unnest_tokens(
 tbl         = wp,
 output      = word,
 input       = txt,
 token       = "words",  # (default) tokenize strings to words
 to_lower    = TRUE,     # (default) set the resulting column to lowercase
 strip_punct = TRUE      # (default) scrub the punctuation from each word
)

# the same line, tokenized into single words (unigrams)
filter(dd_uni, index == 1)
```

So, from an initial data frame, we've gone from a single row per string to rows for each resulting token. The `unnest_tokens()` function is really the key to most of what you'll do with **_tidytext_**, so getting familiar with it is important. Now, because each word is now a row of text, we can start to use existing functions from to do some basic analysis. What if we wanted to track the mentions of terms related to a given topic, like healthcare? Here we'll create a vector of words, and use `cumsum()` to track running totals within each speaker.

```{r echo = FALSE}
theme_set(
  theme_minimal(base_size = 16) +
    theme(
      panel.grid.minor = element_blank(),
      legend.position  = "top"
    )
)
```

```{r}
hc_terms <- c(
  "health", "premium", "premiums",
  "medicare", "sick", "prescription",
  "insurance", "doctor", "medicare", "obamacare"
)

# looking at the top 3 candidates
dd_uni %>%
  filter(speaker %in% c("Biden", "Warren", "Sanders")) %>%
  group_by(speaker) %>%
  mutate(hc = cumsum(word %in% hc_terms)) %>%
  ggplot(aes(x = index, y = hc, color = speaker)) +
  geom_step(size = 1.05) +
  scale_color_scico_d(name = "", palette = "nuuk", direction = -1) +
  labs(
    x = "Remark (debate progress)",
    y = "Mentions",
    title = 'Cumulative mentions of "health", "premium(s)", "medicare",\n"sick", "prescription", "insurance", "doctor",\n"medicare", or "Obamacare"'
  )
```

Here you can see Sanders and Warren responding to questions about their policies/plans during the beginning of the debate, and when Sanders and Biden revisit the topic of the insurance industry near the end.

But, you're definitely not restricted to using just single words! Maybe we want to look for important phrases important to the debate, like "Medicare For All". A phrase like "Medicare For All" can be thought of as what's called an *n-gram*, specifically a *trigram*. N-grams are just a sequence of *n* items from a sample of text or speech; in our case our unit/item is words.

We can use `unnest_tokens()` to pull out all the different trigrams found in our text, and (because the function returns a tidy data frame!) we can then use `dplyr::count()` to see how many times a given trigram is mentioned. 

```{r}
dd_tri <- unnest_tokens(wp, trigram, txt, token = "ngrams", n = 3)

# count the prevalence of all trigrams found
count(dd_tri, trigram, sort = TRUE) %>%
  slice(1:15)
```

From this little exercise, it looks like a transitional phrase, "Thank you Senator" (used mostly by the moderators), is the most frequent trigram. However, something worth noting is that the next most common instance is `NA`-- what's going on here? These simply represent comments that had fewer than 3 words. This is something that we'll see given that we're looking at transcribed speech (as opposed to written responses), so it's important to think about idiosyncracies you might encounter depending on the data you're analyzing.

## handling non-informative words or tokens

In many analyses, it might be important to discard words that aren't useful or helpful to descrbing the data you're working with. In natural language processing, these terms/words are referred to as *stop words*, and they're generally the most common words found in a language. In the English language, a list of stop words would include entries for "I", "is", "of", "are", or "the". There isn't a universally accepted list of words that should be discarded, and it's frequently necessary to augment an existing list with entries that are specific to a given project. For example, we could think about removing the word "senator", given that many of the debate participants are senators and its usage would be similar to how an honorific like "Mr." or "Mrs./Ms." might be used throughout the debate. The **_tidytext_** package helpfully includes several lists of stop words, which are a useful starting place for this task. The helper function `get_stopwords()` enables you to select from a set of 3 lexicons, and supports Spanish, German, and French (in addition to English).

```{r}
smart_stops <- get_stopwords(language = "en", source = "smart")

head(smart_stops)

# here's Cooper's introduction again
# this time we'll drop unigrams that match terms from our list of stop words
dd_uni %>%
  anti_join(smart_stops, by = "word") %>%
  filter(index == 1)
```

Note that because each unigram is stored as a row in the data frame, and each stop word is stored as a row in its own data frame, we're able to discard the ones we don't want by using `dplyr::anti_join()`.

## pulling everything together: quantifying a vocabulary

Next we'll pull each of these preceding steps together to try and identify some distinctive words from some of the leading candidates. One common approach to doing this is by generating TF/IDF scores of given terms. TF/IDF is a combination of *term frequency* and *(inverse) document frequency*.

Term frequency is merely the fraction of times that a term (in this case, word) appears in a given document.

$tf(t,d) = \frac{f_{t,d}}{\sum\limits_{t'\in d} f_{t',d}}$

Document frequency is the number of times a term appears across all documents in a collection. The measure in this case is inverted, so that terms that appear in a small number of documents receive a larger value.

$idf(t,D) = log\frac{N}{1 + |d \in D : t \in d|}$

Lastly, the two measures are combined as a product; terms that are found frequently in a small number of documents get higher scores, whereas terms that are found in virtually every document receive lower scores.

$tfidf(t,d,D) = tf(t, d) \cdot idf(t, D)$

Calculating each of these metrics is straightforward using the `bind_tf_idf()` function. `bind_tf_idf()` simply needs a document ID, a term label, and a count for each term; the results are added to the source data.frame/tibble as new columns. In this case we'll be treating all the text from each speaker in the debate as a document (thus IDF will be high if every speaker uses a given term).

```{r}
# create unigrams, drop stop words, calculate metrics
# then, keep each speaker's top 10 terms
top_10_tfidf <- wp %>%
  unnest_tokens(word, txt) %>%
  anti_join(smart_stops, by = "word") %>%
  count(speaker, word) %>%
  bind_tf_idf(word, speaker, n) %>%
  arrange(speaker, desc(tf_idf)) %>%
  group_by(speaker) %>%
  slice(1:10) %>%
  ungroup()
```

Next we can quickly move on to visualizing these terms/metrics. We'll be using **_ggplot2_** as before, but there are a few extensions in **_tidytext_** worth mentioning, specifically `reorder_within()`. Facets are a super useful aspect of ggplot's repertoire, but I've occasionally found myself struggling to neatly organize factors/categories across panels. I've especially encountered this when working with text and unique tokens that aren't common across different documents. `reorder_within()` handles this within `aes()`, simply taking the vector to be reordered, the metric by which the vector should be sorted, and the group/category that will be used for faceting. The only other thing needed is to add `scale_x/y_reordered()` as an additional layer, and to make sure that the scales for the reordered axis are set as "free" within `facet_wrap()` (or `facet_grid()`).

```{r, fig.width=12, fig.height=8}
top_10_tfidf %>%
  filter(speaker %in% c("Biden", "Warren", "Sanders", "Buttigieg", "Booker", "Harris")) %>%
  ggplot(aes(x = reorder_within(word, tf_idf, speaker), y = tf_idf, fill = speaker)) +
  geom_col() +
  scale_fill_scico_d(palette = "nuuk", direction = -1) +
  scale_x_reordered() +
  facet_wrap(~speaker, scales = "free_y", nrow = 2) +
  labs(x = "", y = "TF/IDF") +
  theme(
    legend.position = "none",
    panel.spacing = unit(1.5, "lines")
  ) +
  coord_flip()
```

## wrap-up

Okay, I think that was a fairly quick overview of some of **_tidytext's_** capabilities, but there's so much more beyond what I've covered here! I really recommend looking at Julia & Dave's book, and I'd like to explore some other analysis methods (such as topic modeling) in the future. Please feel free to let me know if you've found this useful, or if there's something I can better explain. Happy text mining!

<!--
## pulling everything together: tracking sentiment across a text

Next we'll work to actually describe some features of the transcript. In this case, I've decided to demonstrate a really basic sentiment analysis in which we'll track levels of positive/negative words over the course of the debate.

Before anything else, we'll need a way to tell if a given word expresses a positive or negative sentiment. In addition to stop-words, the **_tidytext_** package also includes several datasets that represent lists of previously classified/rated terms for sentiment. We can easily access them using `get_sentiments()`. For simplicity, we'll combine the *Bing* ([Minqing & Bing, 2004](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)), and the [*Loughran-McDonald*](https://sraf.nd.edu/textual-analysis/resources/) lexicons, to get a list of positive/negative terms.

```{r}
bing_sent <- get_sentiments(lexicon = "bing")

# keep the unique positive/negative words from the Loughran dictionary
loug_sent <- get_sentiments(lexicon = "loughran") %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  anti_join(bing_sent, by = "word")

# combine
sent_list <- bind_rows(bing = bing_sent, loughran = loug_sent, .id = "lexicon")

count(sent_list, lexicon, sentiment)
```

This gives us a total of `r scales::comma(nrow(sent_list))` unique terms. It seems like negative terms are more common in both lexicons, but that doesn't necessarily mean that their usage in speech/writing reflects the same prevalence.

Next we'll need to tokenize our data and merge in the sentiment lexicon.

```{r}
dd_sent <- wp %>%
  unnest_tokens(word, txt, token = "words") %>%
  inner_join(sent_list, by = "word")

dd_sent <- dd_sent %>%
  count(speaker, index, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

Again, just like above where we handled stop words, we're able to easily retain only the tokens in our sentiment lexicons via `inner_join()`. 

```{r}
# keeping only remarks from the top 3 candidates
# we'll also recode the index values, so that there aren't breaks between speakers
dd_sent %>%
  filter(speaker %in% c("Warren", "Biden", "Sanders")) %>%
  arrange(index) %>%
  mutate(index = 1:n()) %>%
  ggplot(aes(x = index, y = sentiment, fill = speaker)) +
  geom_col() +
  labs(x = "Comment/Remark #", y = "Sentiment") +
  scale_fill_scico_d(name = "", palette = "nuuk", direction = -1)
```

Here we can see a sharp spikes in negative sentiment when Sanders criticizes American wealth & income inequality (remark 17), and when all 3 discuss/criticize the current administration's handling of Turkey's invasion of Syrian territory (30s~), and at the end of the debate when the candidates were asked to reflect on an impactful friendship. 

-->
