---
title: hey, separation plots are kinda cool
author: Andrew
date: '2019-10-23'
slug: 20191023-separation-plots
tags: ["data viz"]
---



<p>Earlier this week, I was looking around for more ways to spot-check logistic regression models. I’m not sure I fully grok what Pseudo R-Squared statistics mean, and I haven’t seen/heard of an appropriate way to assess residuals from a logistic regression like you can with OLS. In my reading, I came across a plotting technique that’s described in <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-5907.2011.00525.x#b11" target="_blank">this paper</a> (Greenhill, Ward, &amp; Sacks, 2011). They’re called “separation plots”, and they’re used to help assess the fit adequacy of a model that has a binary variable as its dependent variable. (Edit: ugh, I was on university wifi when I was reading and drafted out this post. I didn’t realize the paper was paywalled…now I can’t even reread one of the sections I wanted to quote!)</p>
<p>The authors did create an R package with a <code>separationplot()</code> function, but I found that it was handling R’s graphic devices a little strangely, so I reimplemented a basic version with <strong><em>ggplot2</em></strong>. Anyways, here’s an example plot.</p>
<pre class="r"><code>library(tidyverse)
library(scico)

fit1 &lt;- glm(inmetro ~ percollege, data = midwest, family = binomial)

midwest$f1 &lt;- predict(fit1, midwest, type = &quot;response&quot;)

separation_plot(pred = midwest$f1, actual = midwest$inmetro)</code></pre>
<p><img src="/post/20191023-separation-plots-bhattacharyya-distance_files/figure-html/model%201-1.png" width="864" /></p>
<p>Using the <code>midwest</code> dataset from <strong><em>ggplot2</em></strong>, we’ve built an underspecified model that’s meant to predict whether each county (from 5 midwestern states) is in a metropolitan area, based on the percentage of adults that are college educated. So, what’s going on in this plot? Each vertical line in the figure represents one of the 437 counties, with the yellow lines reflecting <em>events</em> (i.e. instances where <code>inmetro == 1</code>). Each vertical line in the figure is ranked in ascending order based on the predicted probability output by the model. The further a vertical line is to the left, the smaller its estimated probability.</p>
<p>A good model should be able to separate events from non-events, which should translate into vertical lines of the same color clustering together on their respective poles of the plot. For example, let’s look at an improved version of the model we fit earlier.</p>
<pre class="r"><code>fit2 &lt;- glm(
  inmetro ~ percblack + percwhite + percollege + percpovertyknown + percbelowpoverty + percprof,
  data = midwest,
  family = binomial
)

midwest$f2 &lt;- predict(fit2, midwest, type = &quot;response&quot;)

separation_plot(midwest$f2, midwest$inmetro)</code></pre>
<p><img src="/post/20191023-separation-plots-bhattacharyya-distance_files/figure-html/model%202-1.png" width="864" /></p>
<p>In comparison with the first plot, you can see that the non-events on the left are still being broken up occasionally, but these patches are less frequent and smaller. Conversely, on the right it seems like the second model is assigning higher probabilities to counties that really are in metropolitan areas. Let’s also talk about two other pieces of information that are being included in these plots. First, there’s the rising black line that runs the length of the figure. This simply represents the predicted probability of each observation in the model (with 0 at the bottom of the graph, and 1 at the top). As can be seen in the second figure, the slope of the line begins to rise more quickly in relation to the higher density of actual events. Second, there’s the little black triangle just below the lower edge of the plot. This is simply the sum of all the predicted probabilities, and I think is meant to serve as an estimate of the total # of expected events predicted by the model.</p>
<p>In the case of a perfect model, the marker should be exactly at the left-most edge of the events, like in the figure below.</p>
<pre class="r"><code># build a model that perfectly predicts each case
fit3 &lt;- glm(inmetro ~ category, data = midwest, family = binomial)

midwest$f3 &lt;- predict(fit3, midwest, type = &quot;response&quot;)

separation_plot(midwest$f3, midwest$inmetro)</code></pre>
<p><img src="/post/20191023-separation-plots-bhattacharyya-distance_files/figure-html/perfect-1.png" width="864" /></p>
<p>One of the things that feels appealing about this plotting approach is that you can see all the data that’s being modeled. Similar to residual plots, you’re able to get a sense of where fitted values are lying relative to the observed result. It seems like these figures could be an alternative or complement to ROC plots, and I’m curious if other folks find them interesting or useful.</p>
<p>Lastly, here’s the function if you wanted to roll your own ggplot.</p>
<pre class="r"><code>separation_plot &lt;- function(pred, actual, show_legend = FALSE, predline = TRUE, show_expected = TRUE) {
  dat &lt;- tibble(pred = pred, actual = actual) %&gt;%
    arrange(pred) %&gt;%
    mutate(position = 1:n())
  
  p &lt;- dat %&gt;%
    ggplot(aes(x = position, xend = position, y = 0, yend = 1, color = factor(actual))) +
    geom_segment() +
    scale_color_scico_d(palette = &quot;imola&quot;) +
    theme_void() 
  
  if (!show_legend) {
    p &lt;- p + theme(legend.position = &quot;none&quot;)
  }
  
  if (predline) {
    p &lt;- p + geom_line(aes(x = position, y = pred), color = &quot;black&quot;)
  }
  
  if (show_expected) {
    p &lt;- p +
      annotate(geom = &quot;point&quot;, x = max(dat$position) - sum(dat$pred), y = -0.05, shape = 17)
  }
  
  p
}</code></pre>
